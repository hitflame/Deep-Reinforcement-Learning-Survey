{
  "name": "Deep Reinforcement Learning Survey",
  "tagline": "",
  "body": "# Deep Reinforcement Learning survey\r\nThis paper list is a bit different from others. I'll put some opinion and summary on it. However, to understand the whole paper, you still have to read it by yourself!   \r\nSurely, any pull request or discussion are welcomed!\r\n\r\n## Outline\r\n- [Reinforcement Learning Papers](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/tree/master#reinforcement-learning-papers)\r\n- [Open Source](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/tree/master#open-source)\r\n  - Python users\r\n  - Lua users\r\n- [Courses](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/tree/master#course)\r\n- [Textbook](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/tree/master#textbook)\r\n- [Misc](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/tree/master#misc)\r\n\r\n## Reinforcement learning Papers\r\n***Mistakes teach us to clarify what we really want and how we want to live.*** That's the spirit of reinforcement \r\nlearning: learning from the mistakes. Let's be the explorer in reinforcement learning!\r\n\r\n- ***Action-Conditional Video Prediction using Deep Networks in Atari Games*** [[NIPS 2015]](http://arxiv.org/abs/1507.08750)\r\n  - Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh, [[project page]](https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction)\r\n  - Long-term predictions on Atari games conditional on the action\r\n  - Using the predicted frame (more informative) to replace the exploration to improve the model-free controller\r\n  - Multiplicative Action-Conditional Transformation: if use ont-hot to represent the action :point_right: each matrix correspond to a transformation matrix\r\n  - Learning with Multi-Step Prediction (minimize the k steps accumulated error)\r\n  - Section 4.2 is really promising! \r\n    - replace the real frame by predicted frames\r\n    - use prediction model to help agent explore the state visited least\r\n- ***Prioritized Experience Replay*** [[ICML 2016]](http://arxiv.org/abs/1511.05952)\r\n  -  Tom Schaul, John Quan, Ioannis Antonoglou, David Silver\r\n  -  Use prioritized sampling rather than uniformly sampling\r\n  -  Use transition’s TD error δ, which indicates how **\"surprising\" or \"unexpected\" the transition is**\r\n  -  Alleviate the loss of diversity with stochastic prioritization, and introduce bias\r\n  -  Stochastic Prioritization: mixture of pure greedy prioritization and uniform random sampling\r\n- ***Deep Successor Reinforcement Learning*** [[arXiv 2016]](https://arxiv.org/abs/1606.02396)\r\n  - Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman\r\n  - Successor Representation(SR): decomposed the value into successor map and reward predictor (the definition of the components in successor representation should be referred to section 3.2)\r\n  - Advantage1: increase the sensitivityof the environment changes, since it records the immediate reward in each \r\n    state. In DQN, we only record(or predict) the accumulated reward, so the sudden change of reward will be diluted.\r\n    However, the SR mehtod records the reward in each state: R(s), which enable it to be more sensitive to the change\r\n    of environment.\r\n  - Advantage2: able to extract the bottleneck states(subgoals). Since we predict the successor map (the predicted\r\n      visit count), the state with higher visit count is likely to be bottleneck \r\n  - Section 3.3 is a little tricky. The m_sa represents the **feature of the future occupancy**, so m_sa×W becomes \r\n      the future accumulated reward (Q-value). The ∅(s)×W is the **immediate reward**. Therefore, \r\n      m_sa = φ(s) + γE[m_st+1a'] :point_right: eq.6\r\n- ***Deep Reinforcement Learning with Double Q-learning*** [[AAAI 2016]](http://arxiv.org/abs/1509.06461)\r\n  - Hado van Hasselt, Arthur Guez, David Silver \r\n  - Deal with overestimation of Q-values\r\n  - Separate action-select-Q and predict-Q \r\n- ***Playing Atari with Deep Reinforcement Learning*** [[NIPS 2013 Deep Learning Workshop]](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\r\n  - Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves Ioannis Antonoglou, Daan Wierstra  \r\n- ***Human-level control through deep reinforcement learning***, [[Nature 2015]](http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf)\r\n  - Most optimization algorithms assume that the samples are independently and identically distributed,\r\n      while for reinforcement learning, the data is a sequence of action, which breaks the assumption.\r\n  - Strong correlation btn data => break the assumption of stochastic gradient-based algorithms(re-sampling)\r\n  - Experience replay(off-policy)\r\n  - Iterative update Q-value\r\n  - [Source code [Torch]](https://sites.google.com/a/deepmind.com/dqn)\r\n- ***Asynchronous Methods for Deep Reinforcement Learning*** [[ICML 2016]](https://arxiv.org/abs/1602.01783)\r\n  - Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, \r\n      David Silver, Koray Kavukcuoglu \r\n  - On-policy updates\r\n  - Implementation from others:  [async-rl](https://github.com/muupan/async-rl)\r\n  - [Asynchronous SGD](https://cxwangyi.wordpress.com/2013/04/09/why-asynchronous-sgd-works-better-than-its-synchronous-counterpart/), \r\n      explain what \"asynchronous\" means. \r\n  - [Tuning Deep Learning Episode 1: DeepMind's A3C in Torch](http://www.allinea.com/blog/201607/tuning-deep-learning-episode-1-deepminds-a3c-torch)\r\n- ***Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection*** \r\n  [[arXiv 2016]](http://arxiv.org/abs/1603.02199)\r\n  - Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen\r\n  - [Deep Learning for Robots: Learning from Large-Scale Interaction]\r\n      (https://research.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html)\r\n- ***Active Object Localization with Deep Reinforcement Learning*** [[ICCV 2015]](http://arxiv.org/abs/1511.06015)\r\n  - Juan C. Caicedo, Svetlana Lazebnik\r\n  - Agent learns to deform a bounding box using simple transformation action(map the object detection task to RL)   \r\n  - Ideas similar to [G-CNN: an Iterative Grid Based Object Detector](http://arxiv.org/abs/1512.07729)\r\n- ***Dueling Network Architectures for Deep Reinforcement Learning*** [[ICML 2016]](http://arxiv.org/abs/1511.06581)\r\n  - Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas\r\n  - Best Paper in ICML 2016\r\n  - Pose the question: Is conventional CNN suitable for RL tasks?\r\n  - Two stream network(state-value and advantage funvtion)\r\n  - Focusing on innovating a neural network architecture that is better suited for model-free RL\r\n  - Torch blog - [Dueling Deep Q-Networks](http://torch.ch/blog/2016/04/30/dueling_dqn.html)   \r\n- ***Memory-based control with recurrent neural networks*** [[NIPS 2015 Deep Reinforcement Learning Workshop]]\r\n  (http://arxiv.org/abs/1512.04455)\r\n  - Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, David Silver\r\n  - Use RNN to solve partially-observed problem  \r\n- ***Control of Memory, Active Perception, and Action in Minecraft*** [[arXiv 2016]](https://arxiv.org/abs/1605.09128)\r\n  - Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee\r\n  - Solving problem concerning to partial observability\r\n  - Propose mincraft task\r\n  - Memory Q-Network (MQN), Recurrent Memory Q-Network (RMQN), and Feedback Recurrent Memory Q-Network (FRMQN)\r\n- ***Continuous Control With Deep Reinforcement Learning*** [[ICLR 2016]](http://arxiv.org/abs/1509.02971)\r\n  - Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,\r\n     Daan Wierstra\r\n  - Solves the continuous control task, and avoids the curse of **dimension**\r\n  - **Deep** version of DPG(deterministic policy gradient)\r\n  - When going deep, some issues will happens. It's unstable to use the non-linear function to approxiamate \r\n  - The different components of the observation may have different physical units and the ranges may vary \r\n      across environments. => solve by batch normalization\r\n  - For exploration, adding the noise to the actor policy: µ0(st) = µ(st|θt µ) + N\r\n- ***Deterministic Policy Gradient Algorithms*** [[ICML 2014]](http://jmlr.org/proceedings/papers/v32/silver14.pdf)\r\n  - D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller\r\n  - **Highly recommended for learning policy network, and actor-critic algorithms**\r\n  - In continuous action spaces, greedy policy improvement becomes problematic, requiring a global maximisation at\r\n      every step. Instead, a simple and computationally attractive alternative is to move the policy in the direction \r\n      of the gradient of Q, rather than globally maximising Q\r\n- ***Mastering the game of Go with deep neural networks and tree search*** [[Nature 2016]](https://vk.com/doc-44016343_437229031?dl=56ce06e325d42fbc72)\r\n  - David Silver, Aja Huang \r\n  - First stage: supervised learning policy network, including rollout policy and SL policy network(learn the knowledge from human experts)\r\n    -  Rollout policy is used for predicting **fast** but relatively inaccurate decision\r\n    -  SL policy network is used for initialization of RL policy network(improved by policy gradient) \r\n  - To prevent overfit, auto-generate the sample from self-play(half) and train with the KGS dataset(half)\r\n  - Use Monte Carlo tree search with policy network and value network. To understand the MCTS more, plz refer to [here](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search#Principle_of_operation)\r\n    - Selection: select the most promising action depends on Q+u(P) --> depth L\r\n    - Expansion: after L steps, create a new child\r\n    - Evaluation: evaluated by the mixture of value network and simulated rollout\r\n    - Backup: Calculate and store the Q(s,a), N(s,a), which is used in Selection\r\n\r\n## Open Source\r\n### :snake: Python users[Tensorflow, Theano]\r\n  - [OpenAI gym](https://gym.openai.com/)\r\n    - RL **benchmarking** toolkit\r\n    - Provide environment and evaluation metrics\r\n  - [Keras](https://github.com/matthiasplappert/keras-rl)\r\n    - Fully compatible with OpenAI\r\n    - Some algorithms have been implement(e.g DQN, DDQN, DDPG, CDQN)\r\n  - [TensorLayer](https://github.com/zsdonghao/tensorlayer)\r\n    - Built on the top of Google TensorFlow\r\n  - [rllab](https://github.com/rllab/rllab)\r\n    - Fully compatible with OpenAI \r\n    - Continuous control tasks \r\n    - Nice to implement new algorothms\r\n    - [Benchmarking Deep Reinforcement Learning for Continuous Control](https://arxiv.org/abs/1604.06778)\r\n  - [KEras](https://github.com/osh/kerlym)\r\n    - Built on keras\r\n    - Fully compatible with OpenAI\r\n    - Host a handful of agent of reinforcement learning agents\r\n    - [Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent](http://arxiv.org/abs/1605.09221)\r\n  - [Deep Reinforcement Learning in TensorFlow](https://github.com/carpedm20/deep-rl-tensorflow)\r\n    - Implemented by @carpedm20\r\n    - Having some basic reinforcement algorothms\r\n\r\n### :fire: Lua users[Torch]\r\n  - [rltorch](https://github.com/ludc/rltorch), basic reinforcement learning package\r\n  - [awesome-torch for reinforcement learning](https://github.com/carpedm20/awesome-torch#reinforcement-learning)\r\n    - List of open sources for rerinforcement learning \r\n\r\n## Course\r\n  - [CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/#related-materials)\r\n      - Instructors: John Schulman, Pieter Abbeel\r\n  - [UC Berkeley CS188 Intro to AI](http://ai.berkeley.edu/home.html)\r\n      - [2013 Spring video](https://www.youtube.com/user/CS188Spring2013) on youtube   \r\n  - [Advanced Topics: RL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\r\n      - Instructors: David Silver\r\n  - [Deep learning videoes at Oxford 2015](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)\r\n      - Instructors: Nando de Freitas\r\n      - lecture 15, 16 are strongly related to reinforcement learning\r\n  \r\n## Textbook\r\n  - [Foundations_of_Machine_Learning](http://www.cs.nyu.edu/~mohri/mlbook/)\r\n      - chapter 14: Reinforcement learning  \r\n   \r\n  \r\n## Misc\r\n  - [A collection of Deep Learning resources](http://www.jeremydjacksonphd.com/category/deep-learning/)\r\n  - [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/), from Andrej Karpathy' blog\r\n      - policy gradient (very clear!)\r\n  - [Guest Post (Part I): Demystifying Deep Reinforcement Learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/)\r\n  - [Reinforcement Learning and Control](http://cs229.stanford.edu/notes/cs229-notes12.pdf), lecture from Andrew Ng\r\n      - basic reinforcement learning \r\n      - continuous state MDPs\r\n  - [DEEP REINFORCEMENT LEARNING](https://deepmind.com/blog), from David Silver, Google DeepMind\r\n      - briefly discuss some work done by deepmind\r\n  - [What are the benefits of actor/critic framework in reinforcement learning?](https://www.quora.com/What-are-the-benefits-of-actor-critic-framework-in-reinforcement-learning)\r\n      - Clearly expain the advantages of actor/critic \r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}